#!/usr/bin/env python3
"""
Performance Optimization and Scaling System for Tokamak RL Control Suite

This module implements comprehensive performance optimization, caching,
concurrent processing, resource pooling, and auto-scaling capabilities.
"""

import sys
import os
import time
import threading
import multiprocessing as mp
import queue
import asyncio
import concurrent.futures
import weakref
import hashlib
import pickle
from typing import Dict, Any, Optional, List, Callable, Union, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, OrderedDict
from abc import ABC, abstractmethod
import json
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, 'src')

try:
    import numpy as np
except ImportError:
    # Use fallback numpy implementation
    import math
    import random as rand
    
    class np:
        @staticmethod
        def array(x):
            return list(x) if hasattr(x, '__iter__') else [x]
        
        @staticmethod
        def zeros(shape):
            if isinstance(shape, int):
                return [0.0] * shape
            elif len(shape) == 1:
                return [0.0] * shape[0]
            else:
                return [[0.0] * shape[1] for _ in range(shape[0])]
        
        @staticmethod
        def random_random(size=None):
            if size is None:
                return rand.random()
            elif isinstance(size, int):
                return [rand.random() for _ in range(size)]
            else:
                return [[rand.random() for _ in range(size[1])] for _ in range(size[0])]
        
        @staticmethod
        def mean(arr):
            return sum(arr) / len(arr) if arr else 0.0
        
        @staticmethod
        def std(arr):
            if not arr or len(arr) < 2:
                return 0.0
            mean_val = sum(arr) / len(arr)
            return math.sqrt(sum((x - mean_val) ** 2 for x in arr) / len(arr))


@dataclass
class PerformanceMetrics:
    """Performance measurement data."""
    operation: str
    duration: float
    cpu_usage: float
    memory_usage: float
    cache_hit_rate: float
    throughput: float
    timestamp: float
    metadata: Dict[str, Any]


@dataclass
class CacheStats:
    """Cache performance statistics."""
    hits: int
    misses: int
    evictions: int
    size: int
    max_size: int
    hit_rate: float
    memory_usage: float


class LRUCache:
    """High-performance LRU cache implementation."""
    
    def __init__(self, max_size: int = 1000, ttl: Optional[float] = None):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()
        self.timestamps = {}
        self.hits = 0
        self.misses = 0
        self.evictions = 0
        self._lock = threading.RLock()
    
    def _is_expired(self, key: str) -> bool:
        """Check if cache entry is expired."""
        if self.ttl is None:
            return False
        return time.time() - self.timestamps.get(key, 0) > self.ttl
    
    def get(self, key: str) -> Any:
        """Get value from cache."""
        with self._lock:
            if key in self.cache and not self._is_expired(key):
                # Move to end (most recently used)
                value = self.cache.pop(key)
                self.cache[key] = value
                self.hits += 1
                return value
            else:
                if key in self.cache:
                    # Remove expired entry
                    self.cache.pop(key)
                    self.timestamps.pop(key, None)
                self.misses += 1
                return None
    
    def put(self, key: str, value: Any) -> None:
        """Put value in cache."""
        with self._lock:
            if key in self.cache:
                # Update existing
                self.cache.pop(key)
            elif len(self.cache) >= self.max_size:
                # Evict least recently used
                oldest_key = next(iter(self.cache))
                self.cache.pop(oldest_key)
                self.timestamps.pop(oldest_key, None)
                self.evictions += 1
            
            self.cache[key] = value
            self.timestamps[key] = time.time()
    
    def clear(self) -> None:
        """Clear all cache entries."""
        with self._lock:
            self.cache.clear()
            self.timestamps.clear()
    
    def get_stats(self) -> CacheStats:
        """Get cache statistics."""
        with self._lock:
            total_requests = self.hits + self.misses
            hit_rate = self.hits / total_requests if total_requests > 0 else 0.0
            
            # Estimate memory usage
            memory_usage = sys.getsizeof(self.cache) + sys.getsizeof(self.timestamps)
            for key, value in self.cache.items():
                memory_usage += sys.getsizeof(key) + sys.getsizeof(value)
            
            return CacheStats(
                hits=self.hits,
                misses=self.misses,
                evictions=self.evictions,
                size=len(self.cache),
                max_size=self.max_size,
                hit_rate=hit_rate,
                memory_usage=memory_usage
            )


class ComputationCache:
    """Adaptive cache for expensive computations."""
    
    def __init__(self, max_size: int = 500, ttl: float = 300.0):
        self.cache = LRUCache(max_size, ttl)
        self.computation_times = defaultdict(list)
        self._lock = threading.RLock()
    
    def _generate_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """Generate cache key from function signature."""
        # Create hashable representation
        key_data = {
            'func': func_name,
            'args': args,
            'kwargs': sorted(kwargs.items())
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def cached_call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function with caching."""
        func_name = getattr(func, '__name__', str(func))
        cache_key = self._generate_key(func_name, args, kwargs)
        
        # Try cache first
        cached_result = self.cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        # Execute function and measure time
        start_time = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start_time
        
        # Track computation times for adaptive caching
        with self._lock:
            self.computation_times[func_name].append(duration)
            # Keep only recent measurements
            if len(self.computation_times[func_name]) > 100:
                self.computation_times[func_name] = self.computation_times[func_name][-100:]
        
        # Cache if computation was expensive enough
        avg_time = np.mean(self.computation_times[func_name])
        if duration > 0.01 or avg_time > 0.005:  # Cache if > 10ms or avg > 5ms
            self.cache.put(cache_key, result)
        
        return result
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics."""
        cache_stats = self.cache.get_stats()
        
        computation_stats = {}
        with self._lock:
            for func_name, times in self.computation_times.items():
                if times:
                    computation_stats[func_name] = {
                        'avg_time': np.mean(times),
                        'max_time': max(times),
                        'min_time': min(times),
                        'call_count': len(times)
                    }
        
        return {
            'cache_stats': asdict(cache_stats),
            'computation_stats': computation_stats
        }


class ResourcePool:
    """Generic resource pool for expensive objects."""
    
    def __init__(self, resource_factory: Callable, min_size: int = 2, 
                 max_size: int = 10, max_idle_time: float = 300.0):
        self.resource_factory = resource_factory
        self.min_size = min_size
        self.max_size = max_size
        self.max_idle_time = max_idle_time
        
        self.available_resources = queue.Queue()
        self.in_use_resources = set()
        self.resource_creation_times = {}
        self.resource_last_used = {}
        
        self._lock = threading.RLock()
        self._create_initial_resources()
        
        # Start cleanup thread
        self.cleanup_thread = threading.Thread(target=self._cleanup_loop, daemon=True)
        self.cleanup_thread.start()
    
    def _create_initial_resources(self):
        """Create initial pool of resources."""
        for _ in range(self.min_size):
            resource = self.resource_factory()
            self.available_resources.put(resource)
            self.resource_creation_times[id(resource)] = time.time()
    
    def _cleanup_loop(self):
        """Background cleanup of idle resources."""
        while True:
            try:
                time.sleep(60)  # Check every minute
                self._cleanup_idle_resources()
            except Exception as e:
                print(f"Resource pool cleanup error: {e}")
    
    def _cleanup_idle_resources(self):
        """Remove idle resources beyond min_size."""
        current_time = time.time()
        
        with self._lock:
            # Don't cleanup if we're at minimum size
            if self.available_resources.qsize() <= self.min_size:
                return
            
            resources_to_remove = []
            temp_resources = []
            
            # Check each available resource
            while not self.available_resources.empty():
                resource = self.available_resources.get()
                resource_id = id(resource)
                
                last_used = self.resource_last_used.get(resource_id, 
                                                       self.resource_creation_times.get(resource_id, current_time))
                
                if current_time - last_used > self.max_idle_time:
                    resources_to_remove.append(resource)
                else:
                    temp_resources.append(resource)
            
            # Put non-removed resources back
            for resource in temp_resources:
                self.available_resources.put(resource)
            
            # Cleanup metadata for removed resources
            for resource in resources_to_remove:
                resource_id = id(resource)
                self.resource_creation_times.pop(resource_id, None)
                self.resource_last_used.pop(resource_id, None)
    
    def acquire(self, timeout: float = 30.0) -> Any:
        """Acquire resource from pool."""
        try:
            # Try to get from available pool
            resource = self.available_resources.get(timeout=timeout)
            
            with self._lock:
                self.in_use_resources.add(resource)
                self.resource_last_used[id(resource)] = time.time()
            
            return resource
            
        except queue.Empty:
            # Create new resource if under max_size
            with self._lock:
                total_resources = len(self.in_use_resources) + self.available_resources.qsize()
                
                if total_resources < self.max_size:
                    resource = self.resource_factory()
                    self.in_use_resources.add(resource)
                    self.resource_creation_times[id(resource)] = time.time()
                    self.resource_last_used[id(resource)] = time.time()
                    return resource
                else:
                    raise Exception("Resource pool exhausted")
    
    def release(self, resource: Any):
        """Release resource back to pool."""
        with self._lock:
            if resource in self.in_use_resources:
                self.in_use_resources.remove(resource)
                self.available_resources.put(resource)
                self.resource_last_used[id(resource)] = time.time()
    
    def get_stats(self) -> Dict[str, Any]:
        """Get pool statistics."""
        with self._lock:
            return {
                'available_count': self.available_resources.qsize(),
                'in_use_count': len(self.in_use_resources),
                'total_count': self.available_resources.qsize() + len(self.in_use_resources),
                'max_size': self.max_size,
                'min_size': self.min_size
            }


class PerformanceProfiler:
    """Performance profiling and monitoring system."""
    
    def __init__(self, max_samples: int = 10000):
        self.max_samples = max_samples
        self.metrics = defaultdict(list)
        self.active_operations = {}
        self._lock = threading.RLock()
    
    def start_operation(self, operation_id: str, operation_name: str, 
                       metadata: Dict[str, Any] = None) -> float:
        """Start timing an operation."""
        start_time = time.time()
        
        with self._lock:
            self.active_operations[operation_id] = {
                'name': operation_name,
                'start_time': start_time,
                'metadata': metadata or {}
            }
        
        return start_time
    
    def end_operation(self, operation_id: str) -> Optional[PerformanceMetrics]:
        """End timing an operation."""
        end_time = time.time()
        
        with self._lock:
            if operation_id not in self.active_operations:
                return None
            
            operation_info = self.active_operations.pop(operation_id)
            duration = end_time - operation_info['start_time']
            
            metrics = PerformanceMetrics(
                operation=operation_info['name'],
                duration=duration,
                cpu_usage=0.0,  # Would integrate with psutil in full implementation
                memory_usage=0.0,
                cache_hit_rate=0.0,
                throughput=1.0 / duration if duration > 0 else 0.0,
                timestamp=end_time,
                metadata=operation_info['metadata']
            )
            
            # Store metrics
            self.metrics[operation_info['name']].append(metrics)
            
            # Keep only recent samples
            if len(self.metrics[operation_info['name']]) > self.max_samples:
                self.metrics[operation_info['name']] = self.metrics[operation_info['name']][-self.max_samples:]
            
            return metrics
    
    def get_operation_stats(self, operation_name: str) -> Dict[str, Any]:
        """Get statistics for specific operation."""
        with self._lock:
            if operation_name not in self.metrics:
                return {}
            
            operation_metrics = self.metrics[operation_name]
            if not operation_metrics:
                return {}
            
            durations = [m.duration for m in operation_metrics]
            throughputs = [m.throughput for m in operation_metrics]
            
            return {
                'operation': operation_name,
                'sample_count': len(operation_metrics),
                'avg_duration': np.mean(durations),
                'max_duration': max(durations),
                'min_duration': min(durations),
                'std_duration': np.std(durations),
                'avg_throughput': np.mean(throughputs),
                'max_throughput': max(throughputs),
                'recent_duration': durations[-1] if durations else 0.0,
                'last_updated': operation_metrics[-1].timestamp
            }
    
    def get_all_stats(self) -> Dict[str, Any]:
        """Get all operation statistics."""
        with self._lock:
            stats = {}
            for operation_name in self.metrics.keys():
                stats[operation_name] = self.get_operation_stats(operation_name)
            return stats


class ConcurrentExecutor:
    """High-performance concurrent execution manager."""
    
    def __init__(self, max_workers: int = None, thread_pool_size: int = 4, 
                 process_pool_size: int = None):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.thread_pool_size = thread_pool_size
        self.process_pool_size = process_pool_size or min(4, os.cpu_count() or 1)
        
        # Create executor pools
        self.thread_executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=self.thread_pool_size
        )
        self.process_executor = concurrent.futures.ProcessPoolExecutor(
            max_workers=self.process_pool_size
        )
        
        # Task queues for different priority levels
        self.high_priority_queue = queue.PriorityQueue()
        self.normal_priority_queue = queue.PriorityQueue()
        self.low_priority_queue = queue.PriorityQueue()
        
        # Performance tracking
        self.task_count = 0
        self.completed_tasks = 0
        self.failed_tasks = 0
        self.execution_times = []
        
        self._shutdown = False
        self._lock = threading.RLock()
    
    def submit_cpu_intensive(self, func: Callable, *args, **kwargs) -> concurrent.futures.Future:
        """Submit CPU-intensive task to process pool."""
        return self.process_executor.submit(func, *args, **kwargs)
    
    def submit_io_intensive(self, func: Callable, *args, **kwargs) -> concurrent.futures.Future:
        """Submit I/O-intensive task to thread pool."""
        return self.thread_executor.submit(func, *args, **kwargs)
    
    def submit_batch(self, tasks: List[Tuple[Callable, tuple, dict]], 
                    use_processes: bool = False) -> List[concurrent.futures.Future]:
        """Submit batch of tasks."""
        executor = self.process_executor if use_processes else self.thread_executor
        futures = []
        
        for func, args, kwargs in tasks:
            future = executor.submit(func, *args, **kwargs)
            futures.append(future)
        
        return futures
    
    def map_concurrent(self, func: Callable, items: List[Any], 
                      use_processes: bool = False, chunksize: int = 1) -> List[Any]:
        """Map function over items concurrently."""
        executor = self.process_executor if use_processes else self.thread_executor
        
        if use_processes:
            # Process pools support chunksize
            results = list(executor.map(func, items, chunksize=chunksize))
        else:
            # Thread pools don't support chunksize in older Python versions
            futures = [executor.submit(func, item) for item in items]
            results = [future.result() for future in futures]
        
        return results
    
    def shutdown(self, wait: bool = True):
        """Shutdown executor pools."""
        self._shutdown = True
        self.thread_executor.shutdown(wait=wait)
        self.process_executor.shutdown(wait=wait)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get executor statistics."""
        with self._lock:
            return {
                'max_workers': self.max_workers,
                'thread_pool_size': self.thread_pool_size,
                'process_pool_size': self.process_pool_size,
                'task_count': self.task_count,
                'completed_tasks': self.completed_tasks,
                'failed_tasks': self.failed_tasks,
                'success_rate': self.completed_tasks / max(self.task_count, 1),
                'avg_execution_time': np.mean(self.execution_times) if self.execution_times else 0.0
            }


class AutoScaler:
    """Automatic scaling based on load and performance metrics."""
    
    def __init__(self, min_workers: int = 2, max_workers: int = 16, 
                 scale_up_threshold: float = 0.8, scale_down_threshold: float = 0.3,
                 scaling_cooldown: float = 30.0):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.scale_up_threshold = scale_up_threshold
        self.scale_down_threshold = scale_down_threshold
        self.scaling_cooldown = scaling_cooldown
        
        self.current_workers = min_workers
        self.last_scaling_time = 0
        self.load_history = []
        self.scaling_events = []
        
        self._lock = threading.RLock()
    
    def update_load_metrics(self, cpu_usage: float, memory_usage: float, 
                           queue_length: int, response_time: float):
        """Update load metrics for scaling decisions."""
        current_time = time.time()
        
        # Calculate composite load score
        load_score = min(1.0, max(
            cpu_usage,
            memory_usage,
            queue_length / 100.0,  # Normalize queue length
            response_time / 1000.0  # Normalize response time (assume 1s = 1.0)
        ))
        
        with self._lock:
            self.load_history.append({
                'timestamp': current_time,
                'load_score': load_score,
                'cpu_usage': cpu_usage,
                'memory_usage': memory_usage,
                'queue_length': queue_length,
                'response_time': response_time
            })
            
            # Keep only recent history
            cutoff_time = current_time - 300  # 5 minutes
            self.load_history = [entry for entry in self.load_history 
                               if entry['timestamp'] > cutoff_time]
    
    def should_scale(self) -> Tuple[bool, str, int]:
        """Determine if scaling is needed."""
        current_time = time.time()
        
        with self._lock:
            # Check cooldown
            if current_time - self.last_scaling_time < self.scaling_cooldown:
                return False, "cooldown", self.current_workers
            
            if not self.load_history:
                return False, "no_data", self.current_workers
            
            # Calculate average load over recent period
            recent_loads = [entry['load_score'] for entry in self.load_history[-10:]]
            avg_load = np.mean(recent_loads)
            
            # Scale up decision
            if (avg_load > self.scale_up_threshold and 
                self.current_workers < self.max_workers):
                new_workers = min(self.max_workers, self.current_workers + 1)
                return True, "scale_up", new_workers
            
            # Scale down decision
            if (avg_load < self.scale_down_threshold and 
                self.current_workers > self.min_workers):
                new_workers = max(self.min_workers, self.current_workers - 1)
                return True, "scale_down", new_workers
            
            return False, "stable", self.current_workers
    
    def execute_scaling(self, new_worker_count: int, reason: str):
        """Execute scaling operation."""
        with self._lock:
            old_count = self.current_workers
            self.current_workers = new_worker_count
            self.last_scaling_time = time.time()
            
            scaling_event = {
                'timestamp': self.last_scaling_time,
                'old_workers': old_count,
                'new_workers': new_worker_count,
                'reason': reason
            }
            self.scaling_events.append(scaling_event)
            
            # Keep only recent events
            if len(self.scaling_events) > 100:
                self.scaling_events = self.scaling_events[-100:]
    
    def get_scaling_stats(self) -> Dict[str, Any]:
        """Get auto-scaling statistics."""
        with self._lock:
            recent_loads = [entry['load_score'] for entry in self.load_history[-10:]]
            
            return {
                'current_workers': self.current_workers,
                'min_workers': self.min_workers,
                'max_workers': self.max_workers,
                'avg_recent_load': np.mean(recent_loads) if recent_loads else 0.0,
                'scaling_events_count': len(self.scaling_events),
                'last_scaling_time': self.last_scaling_time,
                'time_since_last_scaling': time.time() - self.last_scaling_time
            }


class OptimizedTokamakSystem:
    """High-performance optimized tokamak system."""
    
    def __init__(self):
        # Performance components
        self.computation_cache = ComputationCache(max_size=1000, ttl=600.0)
        self.profiler = PerformanceProfiler(max_samples=50000)
        self.executor = ConcurrentExecutor()
        self.auto_scaler = AutoScaler()
        
        # Resource pools
        self.solver_pool = ResourcePool(
            resource_factory=self._create_physics_solver,
            min_size=2,
            max_size=8
        )
        
        # Performance monitoring
        self.operation_counts = defaultdict(int)
        self.total_operations = 0
        self.startup_time = time.time()
        
        print("üöÄ Optimized Tokamak System initialized")
    
    def _create_physics_solver(self):
        """Factory function for physics solver instances."""
        # In real implementation, this would create actual solver instances
        return {
            'id': f"solver_{time.time()}",
            'initialized': True,
            'last_used': time.time()
        }
    
    def optimized_plasma_simulation(self, state: Dict[str, Any], 
                                   action: List[float]) -> Dict[str, Any]:
        """Optimized plasma simulation with caching and pooling."""
        operation_id = f"sim_{threading.current_thread().ident}_{time.time()}"
        
        # Start performance tracking
        self.profiler.start_operation(operation_id, "plasma_simulation", {
            'state_size': len(state),
            'action_size': len(action)
        })
        
        try:
            # Use cached computation
            result = self.computation_cache.cached_call(
                self._simulate_plasma_step,
                tuple(sorted(state.items())),
                tuple(action)
            )
            
            # Update operation counts
            self.operation_counts['plasma_simulation'] += 1
            self.total_operations += 1
            
            return result
            
        finally:
            # End performance tracking
            metrics = self.profiler.end_operation(operation_id)
            
            # Update auto-scaler with performance metrics
            if metrics:
                self.auto_scaler.update_load_metrics(
                    cpu_usage=0.5,  # Would get from psutil
                    memory_usage=0.3,
                    queue_length=self.operation_counts['plasma_simulation'] % 10,
                    response_time=metrics.duration * 1000
                )
    
    def _simulate_plasma_step(self, state_items: tuple, action: tuple) -> Dict[str, Any]:
        """Core plasma simulation step (cached)."""
        # Simulate expensive computation
        time.sleep(0.01)  # Simulate 10ms computation
        
        # Convert back to dict
        state = dict(state_items)
        
        # Simple plasma evolution simulation
        new_state = state.copy()
        
        # Apply PF coil effects
        pf_effect = sum(action[:6]) * 0.1
        new_state['q_min'] = max(0.5, state.get('q_min', 1.5) + pf_effect)
        
        # Apply heating effects
        heating_effect = action[7] * 0.02
        new_state['plasma_beta'] = min(0.1, state.get('plasma_beta', 0.03) + heating_effect)
        
        # Update shape error
        target_q = 1.8
        q_error = abs(new_state['q_min'] - target_q)
        new_state['shape_error'] = q_error * 5.0
        
        return new_state
    
    def batch_simulate_scenarios(self, scenarios: List[Tuple[Dict, List]]) -> List[Dict]:
        """Batch simulation of multiple scenarios."""
        operation_id = f"batch_sim_{time.time()}"
        
        self.profiler.start_operation(operation_id, "batch_simulation", {
            'scenario_count': len(scenarios)
        })
        
        try:
            # Prepare tasks for concurrent execution
            tasks = []
            for state, action in scenarios:
                tasks.append((
                    self.optimized_plasma_simulation,
                    (state, action),
                    {}
                ))
            
            # Execute concurrently
            futures = self.executor.submit_batch(tasks, use_processes=False)
            results = [future.result() for future in futures]
            
            self.operation_counts['batch_simulation'] += 1
            return results
            
        finally:
            self.profiler.end_operation(operation_id)
    
    def parallel_parameter_sweep(self, base_state: Dict[str, Any], 
                                parameter_ranges: Dict[str, List[float]]) -> Dict[str, List]:
        """Parallel parameter sweep analysis."""
        operation_id = f"param_sweep_{time.time()}"
        
        self.profiler.start_operation(operation_id, "parameter_sweep", {
            'base_state': base_state,
            'parameter_count': len(parameter_ranges)
        })
        
        try:
            # Generate parameter combinations
            scenarios = []
            results = []
            
            for param_name, param_values in parameter_ranges.items():
                for value in param_values:
                    test_state = base_state.copy()
                    test_state[param_name] = value
                    
                    # Default action
                    default_action = [0.0] * 8
                    scenarios.append((test_state, default_action))
            
            # Execute in parallel batches
            batch_size = 10
            for i in range(0, len(scenarios), batch_size):
                batch = scenarios[i:i + batch_size]
                batch_results = self.batch_simulate_scenarios(batch)
                results.extend(batch_results)
            
            # Organize results by parameter
            organized_results = {}
            for param_name, param_values in parameter_ranges.items():
                organized_results[param_name] = []
                
                for i, value in enumerate(param_values):
                    result_index = list(parameter_ranges.keys()).index(param_name) * len(param_values) + i
                    if result_index < len(results):
                        organized_results[param_name].append({
                            'parameter_value': value,
                            'result': results[result_index]
                        })
            
            self.operation_counts['parameter_sweep'] += 1
            return organized_results
            
        finally:
            self.profiler.end_operation(operation_id)
    
    def adaptive_load_balancing(self):
        """Implement adaptive load balancing."""
        should_scale, reason, new_workers = self.auto_scaler.should_scale()
        
        if should_scale:
            self.auto_scaler.execute_scaling(new_workers, reason)
            print(f"üîß Auto-scaling: {reason} to {new_workers} workers")
            
            # In real implementation, would adjust thread/process pools
            if reason == "scale_up":
                # Increase computational resources
                pass
            elif reason == "scale_down":
                # Reduce computational resources
                pass
    
    def optimize_cache_performance(self):
        """Optimize cache performance based on usage patterns."""
        cache_stats = self.computation_cache.get_cache_stats()
        
        # Adjust cache size based on hit rate
        current_hit_rate = cache_stats['cache_stats']['hit_rate']
        
        if current_hit_rate < 0.5 and self.computation_cache.cache.max_size < 2000:
            # Increase cache size for low hit rate
            self.computation_cache.cache.max_size = min(2000, 
                                                       self.computation_cache.cache.max_size * 1.5)
            print(f"üìà Increased cache size to {self.computation_cache.cache.max_size}")
        
        elif current_hit_rate > 0.9 and self.computation_cache.cache.max_size > 100:
            # Decrease cache size for very high hit rate (might be over-provisioned)
            self.computation_cache.cache.max_size = max(100,
                                                       self.computation_cache.cache.max_size * 0.8)
            print(f"üìâ Decreased cache size to {self.computation_cache.cache.max_size}")
    
    def get_comprehensive_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report."""
        uptime = time.time() - self.startup_time
        
        return {
            'system_info': {
                'uptime': uptime,
                'total_operations': self.total_operations,
                'operations_per_second': self.total_operations / max(uptime, 1),
                'operation_counts': dict(self.operation_counts)
            },
            'cache_performance': self.computation_cache.get_cache_stats(),
            'profiler_stats': self.profiler.get_all_stats(),
            'executor_stats': self.executor.get_stats(),
            'auto_scaler_stats': self.auto_scaler.get_scaling_stats(),
            'resource_pool_stats': {
                'solver_pool': self.solver_pool.get_stats()
            },
            'timestamp': time.time()
        }
    
    def shutdown(self):
        """Graceful shutdown of all performance systems."""
        print("üîÑ Shutting down optimized systems...")
        self.executor.shutdown(wait=True)
        print("‚úÖ Optimized Tokamak System shutdown complete")


def run_performance_optimization_demo():
    """Demonstrate comprehensive performance optimization capabilities."""
    print("‚ö° Starting Performance Optimization Demo")
    print("=" * 50)
    
    # Initialize optimized system
    system = OptimizedTokamakSystem()
    
    # Test basic optimized simulation
    print("\nüî¨ Testing Optimized Plasma Simulation...")
    test_state = {
        'plasma_current': 10.0,
        'plasma_beta': 0.03,
        'q_min': 1.6,
        'shape_error': 2.0
    }
    test_action = [0.1, -0.1, 0.2, -0.05, 0.0, 0.15, 0.5, 0.3]
    
    # Run simulation multiple times to test caching
    for i in range(5):
        start_time = time.time()
        result = system.optimized_plasma_simulation(test_state, test_action)
        duration = (time.time() - start_time) * 1000
        print(f"  Simulation {i+1}: {duration:.2f}ms, q_min={result['q_min']:.3f}")
    
    # Test batch simulation
    print("\nüìä Testing Batch Simulation...")
    scenarios = []
    for i in range(20):
        state = test_state.copy()
        state['plasma_current'] = 8.0 + i * 0.5
        action = [x + 0.01 * i for x in test_action]
        scenarios.append((state, action))
    
    start_time = time.time()
    batch_results = system.batch_simulate_scenarios(scenarios)
    batch_duration = (time.time() - start_time) * 1000
    print(f"  Batch of {len(scenarios)} scenarios: {batch_duration:.2f}ms total")
    print(f"  Average per scenario: {batch_duration/len(scenarios):.2f}ms")
    
    # Test parameter sweep
    print("\nüîç Testing Parallel Parameter Sweep...")
    parameter_ranges = {
        'plasma_current': [8.0, 10.0, 12.0, 14.0],
        'plasma_beta': [0.02, 0.03, 0.04, 0.05]
    }
    
    start_time = time.time()
    sweep_results = system.parallel_parameter_sweep(test_state, parameter_ranges)
    sweep_duration = (time.time() - start_time) * 1000
    print(f"  Parameter sweep: {sweep_duration:.2f}ms")
    
    for param_name, results in sweep_results.items():
        print(f"    {param_name}: {len(results)} parameter values tested")
    
    # Test auto-scaling
    print("\nüîß Testing Auto-Scaling...")
    for i in range(3):
        # Simulate varying load
        system.auto_scaler.update_load_metrics(
            cpu_usage=0.3 + i * 0.3,
            memory_usage=0.2 + i * 0.2,
            queue_length=i * 5,
            response_time=50 + i * 100
        )
        system.adaptive_load_balancing()
        time.sleep(0.1)
    
    # Test cache optimization
    print("\nüíæ Testing Cache Optimization...")
    system.optimize_cache_performance()
    
    # Generate comprehensive performance report
    print("\nüìà Performance Report:")
    report = system.get_comprehensive_performance_report()
    
    print(f"  System Uptime: {report['system_info']['uptime']:.2f}s")
    print(f"  Total Operations: {report['system_info']['total_operations']}")
    print(f"  Operations/sec: {report['system_info']['operations_per_second']:.2f}")
    
    cache_stats = report['cache_performance']['cache_stats']
    print(f"  Cache Hit Rate: {cache_stats['hit_rate']:.1%}")
    print(f"  Cache Size: {cache_stats['size']}/{cache_stats['max_size']}")
    
    scaling_stats = report['auto_scaler_stats']
    print(f"  Current Workers: {scaling_stats['current_workers']}")
    print(f"  Average Load: {scaling_stats['avg_recent_load']:.2f}")
    
    solver_pool = report['resource_pool_stats']['solver_pool']
    print(f"  Solver Pool: {solver_pool['available_count']} available, {solver_pool['in_use_count']} in use")
    
    # Test performance under load
    print("\nüöÄ Load Testing...")
    load_scenarios = []
    for i in range(100):
        state = test_state.copy()
        state['plasma_current'] = 5.0 + i * 0.1
        action = [x * (1 + 0.01 * i) for x in test_action]
        load_scenarios.append((state, action))
    
    start_time = time.time()
    load_results = system.batch_simulate_scenarios(load_scenarios)
    load_duration = (time.time() - start_time) * 1000
    
    print(f"  Load test ({len(load_scenarios)} scenarios): {load_duration:.2f}ms")
    print(f"  Throughput: {len(load_scenarios) / (load_duration / 1000):.1f} scenarios/second")
    
    # Shutdown
    system.shutdown()
    
    print("\nüéØ Performance Optimization Demo Complete!")
    print("‚úì Caching system tested")
    print("‚úì Concurrent execution tested")
    print("‚úì Auto-scaling tested")
    print("‚úì Resource pooling tested")
    print("‚úì Performance profiling tested")
    print("‚úì Load balancing tested")


if __name__ == "__main__":
    run_performance_optimization_demo()